{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/yash/Desktop/AIREX/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yash/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.85s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from accelerate import infer_auto_device_map, dispatch_model\n",
    "import torch\n",
    "\n",
    "# Define the model path\n",
    "model_path = \"ibm-granite/granite-3.1-2b-base\"  # Replace with your model path\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load the model weights directly onto devices\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# Define memory constraints for GPU and CPU\n",
    "if torch.cuda.is_available():\n",
    "    max_memory = {0: \"3.5GB\", \"cpu\": \"16GB\"}  # Use integer for GPU index\n",
    "else:\n",
    "    max_memory = {\"cpu\": \"16GB\"}  # Adjust based on your system's RAM\n",
    "\n",
    "# Generate the device map for layer distribution\n",
    "device_map = infer_auto_device_map(\n",
    "    model, \n",
    "    max_memory=max_memory,\n",
    "    no_split_module_classes=[\"GPTNeoXLayer\"]\n",
    ")\n",
    "\n",
    "# Dispatch the model with weights\n",
    "model = dispatch_model(model, device_map=device_map)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write a function to calculate the factorial of a number in python.\n",
      "\n",
      "Here's a Python function to calculate the factorial of a number:\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * factorial(n-1)\n",
      "```\n",
      "\n",
      "Let's test the function with an example:\n",
      "\n",
      "```python\n",
      "print(factorial(5))\n",
      "```\n",
      "\n",
      "Output:\n",
      "\n",
      "```\n",
      "120\n",
      "```\n",
      "\n",
      "This function uses recursion to calculate the factorial of a number. It checks if the input number `n` is equal to 0, and if so, returns 1 (since the factorial of 0 is 1). Otherwise, it multiplies the input number `n` by the factorial of `n-1` and returns the result.\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "120\n"
     ]
    }
   ],
   "source": [
    "# Example input\n",
    "input_text = \"write a function to calculate the factorial of a number in python\"\n",
    "input_tokens = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Ensure input tokens are on the correct device\n",
    "input_tokens = {key: value.to(next(model.parameters()).device) for key, value in input_tokens.items()}\n",
    "\n",
    "# Generate output\n",
    "output_tokens = model.generate(**input_tokens, max_length=200)\n",
    "output_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the result\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All VTK files have been converted to .npy files.\n"
     ]
    }
   ],
   "source": [
    "import pyvista as pv\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def vtk_to_npy(file_path, output_dir=\"./npy_data/\"):\n",
    "    \"\"\"\n",
    "    Convert VTK file data to .npy format and save.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the VTK file.\n",
    "        output_dir (str): Directory to save the .npy files.\n",
    "    \"\"\"\n",
    "    # Read the VTK file\n",
    "    grid = pv.read(file_path)\n",
    "    points = grid.points  # Extract spatial points\n",
    "    temperature = grid[\"Temperature\"]  # Extract temperature field\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract the file name without extension\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    \n",
    "    # Save points and temperature as .npy files\n",
    "    np.save(os.path.join(output_dir, f\"{base_name}_points.npy\"), points)\n",
    "    np.save(os.path.join(output_dir, f\"{base_name}_temperature.npy\"), temperature)\n",
    "\n",
    "# Process all VTK files\n",
    "vtk_files = glob.glob(\"./working_2/*.vtk\")\n",
    "for file in vtk_files:\n",
    "    vtk_to_npy(file)\n",
    "\n",
    "print(\"All VTK files have been converted to .npy files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "\n",
    "def npy_to_text(data_dir=\"./npy_data/\", output_file=\"training_data.txt\"):\n",
    "    points_files = sorted(glob.glob(f\"{data_dir}/*_points.npy\"))\n",
    "    temperature_files = sorted(glob.glob(f\"{data_dir}/*_temperature.npy\"))\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for points_file, temp_file in zip(points_files, temperature_files):\n",
    "            points = np.load(points_file).tolist()\n",
    "            temperature = np.load(temp_file).tolist()\n",
    "            data_entry = {\"points\": points, \"temperature\": temperature}\n",
    "            f.write(json.dumps(data_entry) + \"\\n\")\n",
    "\n",
    "# Generate the text dataset\n",
    "npy_to_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100 examples [00:00, 299.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": \"training_data.txt\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)  # Use the fast tokenizer if available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:38<00:00,  2.60 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize in small batches\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    batch_size=32  # Adjust batch size based on your system's capacity\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DatasetDict' object has no attribute 'train_test_split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DatasetDict\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Split dataset into train and validation sets\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m dataset_split \u001b[38;5;241m=\u001b[39m \u001b[43mtokenized_datasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_test_split\u001b[49m(test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Create a data collator for language modeling\u001b[39;00m\n\u001b[1;32m      8\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForLanguageModeling(\n\u001b[1;32m      9\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     10\u001b[0m     mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Set to True if doing masked language modeling\u001b[39;00m\n\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DatasetDict' object has no attribute 'train_test_split'"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# Split dataset into train and validation sets\n",
    "dataset_split = tokenized_datasets.train_test_split(test_size=0.1)\n",
    "\n",
    "# Create a data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Set to True if doing masked language modeling\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
